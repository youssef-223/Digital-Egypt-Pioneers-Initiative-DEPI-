import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import subprocess
from IPython.display import display
from google.colab import files
import os
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder



# Usage:
# setup_kaggle_competition('microsoft-malware-prediction')
def setup_kaggle_competition(competition_name, kaggle_json='kaggle.json'):
    # Define paths
    kaggle_json_path = os.path.join(os.getcwd(), kaggle_json)
    kaggle_dir = os.path.expanduser('~/.kaggle')
    kaggle_json_dest = os.path.join(kaggle_dir, kaggle_json)

    # Step 1: Check if 'kaggle.json' exists in the current directory
    if not os.path.isfile(kaggle_json_path):
        print(f"'{kaggle_json}' not found in the current directory.")
        print("Please upload your 'kaggle.json' file.")
        uploaded = files.upload()
        
        # Verify upload
        if kaggle_json not in uploaded:
            raise FileNotFoundError(f"{kaggle_json} was not uploaded.")
        
        # Move the uploaded file to the correct location
        os.rename(kaggle_json, kaggle_json_path)
    
    # Step 2: Create Kaggle directory if it does not exist and move 'kaggle.json'
    if not os.path.exists(kaggle_dir):
        os.makedirs(kaggle_dir)
    
    # Move the Kaggle JSON file to the Kaggle directory
    if not os.path.isfile(kaggle_json_dest):
        os.rename(kaggle_json_path, kaggle_json_dest)
        os.chmod(kaggle_json_dest, 0o600)
        print(f"Moved '{kaggle_json}' to '{kaggle_dir}'.")

    # Step 3: Download the competition dataset
    print(f"Downloading competition data for '{competition_name}' from Kaggle.")
    result = subprocess.run(['kaggle', 'competitions', 'download', '-c', competition_name],
                            capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error downloading data: {result.stderr}")
        return
    
    print("Download successful.")

    # Step 4: Unzip the dataset
    zip_file = f"{competition_name}.zip"
    if os.path.exists(zip_file):
        print(f"Unzipping the '{zip_file}' file.")
        result = subprocess.run(['unzip', zip_file], capture_output=True, text=True)
        if result.returncode != 0:
            print(f"Error unzipping file: {result.stderr}")
        else:
            print("Unzip successful.")
    else:
        print(f"Zip file '{zip_file}' not found.")

# Usage
# random_sample = load_random_sample('train.csv', sample_size=10000, random_seed=42)

def load_random_sample(file_path, sample_size=10000, random_seed=None):
    # Set the random seed if provided
    if random_seed is not None:
        random.seed(random_seed)
    
    # Get the number of rows in the file
    row_count = sum(1 for _ in open(file_path)) - 1  # Subtract 1 to account for the header
    
    # Generate random row numbers to skip (to achieve random sampling)
    skip_rows = sorted(random.sample(range(1, row_count + 1), row_count - sample_size))
    
    # Load the sample (skip the randomly selected rows)
    random_sample = pd.read_csv(file_path, skiprows=skip_rows)
    
    # Return the random sample
    return random_sample



def null_summary(df):
    # Create the DataFrame with missing values summary
    missing_values_summary = pd.DataFrame({
        'Total No. of Missing Values': df.isnull().sum(),
        'Percentage of Missing Values': (df.isnull().sum() / len(df)) * 100
    })

    # Display the DataFrame
    display(missing_values_summary)


def clean_and_impute_data(df, cat_cols, num_cols):

    # Calculate the percentage of missing values for each feature
    missing_percentage = (df.isnull().sum() / len(df)) * 100

    # Identify features with more than 50% missing values
    features_to_drop = missing_percentage[missing_percentage > 50].index

    # Drop the features with more than 50% missing values
    df_cleaned = df.drop(columns=features_to_drop)

    # Print the names of the dropped features
    print("Dropped features with more than 50% missing values:")
    print(features_to_drop.tolist())

    # Update cat_cols and num_cols by removing the dropped features
    cat_cols = [col for col in cat_cols if col not in features_to_drop]
    num_cols = [col for col in num_cols if col not in features_to_drop]

    # Impute missing values for categorical features with mode
    imputer_cat = SimpleImputer(strategy='most_frequent')
    df_cleaned[cat_cols] = imputer_cat.fit_transform(df_cleaned[cat_cols])

    # Impute missing values for numerical features with mean
    imputer_num = SimpleImputer(strategy='mean')
    df_cleaned[num_cols] = imputer_num.fit_transform(df_cleaned[num_cols])

    # Display the old shape of the Dataframe for clarrification
    print(f"\nOriginal DataFrame shape: {df.shape}")
    # Display the cleaned DataFrame shape for confirmation
    print(f"New DataFrame shape: {df_cleaned.shape}")

    return df_cleaned, cat_cols, num_cols



def plot_numerical_features(data, features_per_row=5):
    numerical_data = data.select_dtypes(include=['int64', 'float64'])
    num_features = len(numerical_data.columns)
    
    num_rows = (num_features + features_per_row - 1) // features_per_row
    
    for row in range(num_rows):
        start_col = row * features_per_row
        end_col = min(start_col + features_per_row, num_features)
        cols_to_plot = numerical_data.columns[start_col:end_col]
        
        fig, axes = plt.subplots(1, len(cols_to_plot), figsize=(20, 5))  # Increased figure width
        if len(cols_to_plot) == 1:
            axes = [axes]  # Ensure axes is iterable if only one subplot
        
    for i, col in enumerate(cols_to_plot):
        sns.histplot(numerical_data[col], kde=True, ax=axes[i])
        axes[i].set_title(f'Distribution of {col}', fontsize=12)
        axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability
        
        plt.tight_layout()
        plt.show()

        
def get_cat_cols(df, max_unique=10, show_unique_values=False):
    # Initialize sets to hold column names
    numerical_with_few_unique = set()
    all_object_columns = set()

    # Iterate over columns to check unique values
    for column in df.columns:
        # Check for numerical columns with fewer than max_unique unique values
        if pd.api.types.is_numeric_dtype(df[column]):
            num_unique = len(df[column].unique())
            if num_unique < max_unique:
                numerical_with_few_unique.add(column)
        
        # Check for object type columns
        elif pd.api.types.is_object_dtype(df[column]):
            all_object_columns.add(column)
    
    # Combine both sets into one list
    combined_columns = list(numerical_with_few_unique.union(all_object_columns))
    
    # Print the results for verification
    if show_unique_values:
        print("Numerical columns with fewer than", max_unique, "unique values:")
        show_uniqeness(df,numerical_with_few_unique)
    
        print("All categorical columns with dtype 'object':")
        show_uniqeness(df,all_object_columns)
    
    # Return the combined list of columns
    return combined_columns




def countplot(df, column, target):
    plt.figure(figsize=(15,5))
    ax = sns.countplot(x=column, data=df, hue=target ,palette="Set2")
    for value in ax.patches:
        percentage = "{:.1f}%".format(100*value.get_height()/len(df[column]))
        x = value.get_x() + value.get_width() / 2 - 0.05
        y = value.get_y() + value.get_height()
        ax.annotate(percentage, (x,y), fontweight="black",size=15)
        
    plt.title(f"Gamer devices by {column}",fontweight="black",size=20,pad=20)
    plt.show()


def continous_plot(df, column, target):
    plt.figure(figsize=(13,6))
    plt.subplot(1,2,1)
    sns.histplot(x=column,hue=target,data=df,kde=True,palette="Set2")
    plt.title(f"Distribution of '{column}' by '{target}' Status",fontweight="black",pad=20,size=15)

    plt.subplot(1,2,2)
    sns.boxplot(y= column,data=df,hue=target ,palette="Set2")
    plt.tight_layout()
    plt.show()



def downcast_numerical(df):
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:  # Check if column is numerical
            df[col] = pd.to_numeric(df[col], downcast='integer')
    return df


def encode_categorical(df, cat_cols):
    label_encoders = {}
    
    for col in cat_cols:
        if col in df.columns:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            label_encoders[col] = le
    
    return df


def show_uniqeness(df,cols):
    for column in cols:
        unique_values = df[column].unique()
        print(f"Column: '{column}'")
        print(f"Number of unique values: {len(unique_values)}")
        print(f"Unique values: {unique_values}")
        print("-" * 100, "\n")


def handle_outliers_and_correlations(df, numeric_cont_cols, target_col, z_threshold=3, iqr_factor=1.5, outlier_threshold=10, corr_threshold=0.1):
    """
    Handle outliers using Z-score for near-normal distributions and IQR for skewed distributions.
    Drop columns with high percentages of outliers and low correlation with the target variable.

    Parameters:
    df (pd.DataFrame): The dataframe containing the data.
    numeric_cont_cols (list): A list of column names that are continuous and numeric.
    target_col (str): The target variable name.
    z_threshold (float): The threshold for Z-score to detect outliers. Default is 3.
    iqr_factor (float): The multiplication factor for IQR outlier detection. Default is 1.5.
    outlier_threshold (float): The minimum percentage of outliers required to consider removing a column. Default is 10%.
    corr_threshold (float): The correlation threshold below which a column is dropped. Default is 0.1.

    Returns:
    df_cleaned (pd.DataFrame): The cleaned dataframe with outliers handled and low correlation columns removed.
    dropped_columns (list): A list of columns dropped due to low correlation with the target.
    """

    # Color formatting for console output
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

    # Calculate skewness of the columns
    skewed_cols = df[numeric_cont_cols].apply(skew)
    
    outliers_zscore = pd.DataFrame()
    outliers_IQR = pd.DataFrame()
    outlier_columns = []
    
    for col in numeric_cont_cols:
        if np.abs(skewed_cols[col]) < 0.5:
            # Use Z-score for near-normal distributions
            z_scores = np.abs(zscore(df[col]))
            outliers_rows = df.loc[z_scores > z_threshold, col]
            outliers_zscore = pd.concat([outliers_zscore, outliers_rows], axis=1)
        else:
            # Use IQR method for skewed distributions
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers_col = df.loc[(df[col] < (Q1 - iqr_factor * IQR)) | (df[col] > (Q3 + iqr_factor * IQR)), col]
            outliers_IQR = pd.concat([outliers_IQR, outliers_col], axis=1)
    
    # Combine outlier indices from both methods
    outlier_indices_zscore = outliers_zscore.index
    outlier_indices_IQR = outliers_IQR.index
    total_outlier_indices = outlier_indices_zscore.union(outlier_indices_IQR)

    # Calculate the percentage of outliers for each column
    outlier_percentage = {}
    for col in numeric_cont_cols:
        outliers_in_col = df.loc[total_outlier_indices, col]
        outlier_percentage[col] = (len(outliers_in_col.dropna()) / len(df)) * 100

    # Check correlation and drop columns with high outliers and low correlation
    dropped_columns = []
    retained_columns = list(numeric_cont_cols)  # Keep track of columns not dropped

    for col, percentage in outlier_percentage.items():
        correlation = df[col].corr(df[target_col])

        # Highlight high outlier percentages and low correlations
        outlier_msg = f"{BOLD + RED if percentage > outlier_threshold else ''}{percentage:.2f}%{RESET}"
        corr_msg = f"{BOLD + RED if np.abs(correlation) < corr_threshold else ''}{correlation:.3f}{RESET}"

        print(f"Column: '{col}', Outlier %: {outlier_msg}, Correlation with {target_col}: {corr_msg}")
        
        if percentage > outlier_threshold and np.abs(correlation) < corr_threshold:
            print("-" * 100)
            print(f"{YELLOW}>> Dropping column '{col}' due to low correlation with {target_col}.{RESET}")
            print("-" * 100, "\n")
            df = df.drop(columns=[col])
            dropped_columns.append(col)
            retained_columns.remove(col)

    # Recalculate the outlier indices for retained columns only
    retained_outlier_indices = pd.Index([])
    for col in retained_columns:
        if np.abs(skewed_cols[col]) < 0.5:
            z_scores = np.abs(zscore(df[col]))
            retained_outlier_indices = retained_outlier_indices.union(df.loc[z_scores > z_threshold, col].index)
        else:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            retained_outlier_indices = retained_outlier_indices.union(df.loc[(df[col] < (Q1 - iqr_factor * IQR)) | (df[col] > (Q3 + iqr_factor * IQR)), col].index)

    # Drop outlier rows for retained columns only
    df_cleaned = df.drop(retained_outlier_indices)
    print(f"The shape of the new data : {df_cleaned.shape}")

    return df_cleaned, dropped_columns


def remove_cols_by_skeweness(df, target_col, skewness_threshold=10, correlation_threshold=0.11):
    """
    Remove columns from the dataframe if their skewness is greater than the given threshold and 
    their correlation with the target variable is less than the given threshold.
    
    Parameters:
    df (pd.DataFrame): The dataframe containing the data.
    target_col (str): The target variable name.
    skewness_threshold (float): The threshold for skewness to consider a column for removal. Default is 10.
    correlation_threshold (float): The threshold for correlation with the target variable below which a column is removed. Default is 0.11.
    
    Returns:
    pd.DataFrame: The dataframe with columns removed based on the skewness and correlation criteria.
    """
    # Calculate skewness for each column
    skewness = df.apply(lambda x: skew(x.dropna()))
    
    # Calculate correlation with the target variable
    correlations = df.corr()[target_col].abs()
    
    # Identify columns to drop
    columns_to_drop = []
    for col in df.columns:
        if abs(skewness[col]) > skewness_threshold and abs(correlations[col]) < correlation_threshold:
            columns_to_drop.append(col)
    
    # Drop the identified columns
    df_cleaned = df.drop(columns=columns_to_drop)
    
    print(f"Columns removed due to high skewness (>{skewness_threshold}) and low correlation (<{correlation_threshold}) with {target_col}: {columns_to_drop}")
    print(f"Shape of the dataframe after removal: {df_cleaned.shape}")
    
    return df_cleaned
